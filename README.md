# ğŸ“¦ MLOps-Telecom-Churn-Detection

**An advanced end-to-end **MLOps pipeline** for Telecom Customer Churn Prediction using **ZenML**, **MLflow**, **Docker**, **GitHub Actions**, and **AWS EC2**. This project automates everything from data ingestion, preprocessing, model training & evaluation, deployment, drift detection, and CI/CD.**

[![ZenML](https://img.shields.io/badge/ZenML-Orchestration-6D4AFF?style=flat&logo=zenml)](https://zenml.io/)
[![MLflow](https://img.shields.io/badge/MLflow-Tracking-0194E2?style=flat&logo=mlflow)](https://mlflow.org/)
[![Docker](https://img.shields.io/badge/Docker-Containerization-2496ED?style=flat&logo=docker)](https://www.docker.com/)
[![Streamlit](https://img.shields.io/badge/Streamlit-UI-FF4B4B?style=flat&logo=streamlit)](https://streamlit.io/)
[![GitHub Actions](https://img.shields.io/badge/GitHub%20Actions-CI%2FCD-2088FF?style=flat&logo=github-actions)](https://github.com/features/actions)
[![AWS EC2](https://img.shields.io/badge/AWS-EC2-F29111?style=flat&logo=amazon-aws)](https://aws.amazon.com/ec2/)
[![DynamoDB](https://img.shields.io/badge/DynamoDB-Database-4053D6?style=flat&logo=amazon-dynamodb)](https://aws.amazon.com/dynamodb/)
[![Evidently](https://img.shields.io/badge/Evidently-Drift%20Detection-00B5AD?style=flat)](https://evidentlyai.com/)
[![Pytest](https://img.shields.io/badge/Pytest-Testing-0A9EDC?style=flat&logo=pytest)](https://docs.pytest.org/)
[![Python](https://img.shields.io/badge/Python-3.10+-3776AB?style=flat&logo=python)](https://www.python.org/)

---

## ğŸ§  Project Overview

**Goal:** Predict whether a telecom customer will churn based on customer and service attributes.

**Key Features:**

* End-to-end modular MLOps architecture
* ZenML pipelines for orchestration
* MLflow tracking
* Dockerized deployment with Streamlit
* Drift detection & auto-retraining via GitHub Actions
* CI/CD: Testing, Docker Build & Deployment via GitHub Actions
* Hosted on AWS EC2
* DynamoDB integration for storing inference data

---

## ğŸ“‚ Project Structure

```
MLOps-Telecom-Churn-Detection/
â”œâ”€â”€ .github/workflows/              # CI/CD pipeline using GitHub Actions
â”‚   â””â”€â”€ cicd.yml                    # Automates tests, Docker build, and deployment to EC2
â”‚   â””â”€â”€ drift_detection.yml         # Workflow for scheduled drift detection and retraining
â”‚
â”œâ”€â”€ data/    
â”‚   â”œâ”€â”€ raw/                        # Original Telco churn CSV file
â”‚   â”œâ”€â”€ processed/                  # Cleaned and split data: train.csv, val.csv, test.csv
â”‚   â””â”€â”€ reference/                  # Stores reference data for drift detection       
â”‚
â”œâ”€â”€ model/                             # Trained and serialized model files
â”‚   â”œâ”€â”€ best_model.pkl             # Best selected model
â”‚   â”œâ”€â”€ transformer.pkl            # Preprocessing transformer object
â”‚   â””â”€â”€ [other models].pkl         # Additional model candidates (XGBoost, Random Forest, and Logistic Regression.)
â”‚
â”œâ”€â”€ artifacts/                         # Model evaluation results and visuals
â”‚   â””â”€â”€ evaluation/                # Includes plots and reports
â”‚       â”œâ”€â”€ roc_curve_<model>.png
â”‚       â”œâ”€â”€ precision_recall_curve_<model>.png
â”‚       â”œâ”€â”€ confusion_matrix_<model>.png
â”‚       â”œâ”€â”€ classification_report_<model>.txt
â”‚       â”œâ”€â”€ # Other metrics
â”‚
â”œâ”€â”€ data_analysis/ 
â”‚   â”œâ”€â”€ eda.ipynb                  # Jupyter Notebook to demonstrate EDA
â”‚   â”œâ”€â”€ initial_inspection.py
â”‚   â”œâ”€â”€ univariate_analysis.py
â”‚   â”œâ”€â”€ bivariate_analysis.py
â”‚   â”œâ”€â”€ multivariate_analysis.py
â”‚   â”œâ”€â”€ missing_values_analysis.py
â”‚
â”œâ”€â”€ pipelines/                         # ZenML pipeline definitions
â”‚   â””â”€â”€ training_pipeline.py       # Main ZenML training + deployment pipeline
â”‚
â”œâ”€â”€ src/                               # Core pipeline components and logic
â”‚   â”œâ”€â”€ config.py                  # Global config variables and paths
â”‚   â”œâ”€â”€ utils.py                  # utils for logging 
â”‚   â”œâ”€â”€ data_ingestion.py         # Raw dataset reading logic
â”‚   â”œâ”€â”€ preprocessing.py          # Data cleaning and transformation
â”‚   â”œâ”€â”€ feature_engineering.py    # Feature creation and encoding
â”‚   â”œâ”€â”€ splitting.py              # Splits the dataset into train/val/test
â”‚   â”œâ”€â”€ training.py               # Trains multiple models
â”‚   â”œâ”€â”€ evaluation.py             # Generates classification metrics and plots
â”‚   â”œâ”€â”€ deployment.py             # Deploys a Streamlit app to EC2 using Docker
â”‚   â”œâ”€â”€ drift_detection.py        # Drift detection
â”‚   â””â”€â”€ aws_database.py           # AWS database functions
â”‚
â”œâ”€â”€ tests/                             # Unit and integration tests using Pytest
â”‚   â”œâ”€â”€ test_data_validation.py
â”‚   â”œâ”€â”€ test_feature_engineering.py
â”‚   â”œâ”€â”€ test_model_validation.py
â”‚   â”œâ”€â”€ test_preprocessing.py
â”‚   â”œâ”€â”€ test_training.py
â”‚   â””â”€â”€ test_smoke.py
â”‚
â”œâ”€â”€ last_processed.txt                 # Last drift detection date
â”œâ”€â”€ run_pipeline.py                    # Entry point to trigger the ZenML pipeline
â”œâ”€â”€ Dockerfile                         # Builds the Docker container for app deployment
â”œâ”€â”€ requirements.txt                   # All Python dependencies
â”œâ”€â”€ deploy_requirements.txt            # All Python dependencies required for deployment
â”œâ”€â”€ README.md                          # Project documentation
â””â”€â”€ LICENSE                             # Project licensing
```

---

## ğŸ“Š Data

**Source:** [Telco Customer Churn Dataset](https://www.kaggle.com/datasets/blastchar/telco-customer-churn)

* Target: `Churn`
* Mix of categorical and numerical features

Data is saved in `data/raw/Telco-Customer-Churn.csv`, and processed via ZenML ingestion steps.

---

## ğŸ” EDA (Exploratory Data Analysis)

* Found class imbalance in churn target
* Features like tenure, monthly charges, and contract type are key drivers
* Handled:

  * Missing values
  * Outliers
  * Skewness
  * Binning tenure

**Visualizations:**

* Histograms, Boxplots
* Countplots for categorical variables
* Correlation matrices & pairplots
* ROC, PR curves

EDA plots are saved under `artifacts/evaluation/`

---

## ğŸ”„ MLOps Workflow

**Pipeline Steps:**

1. ğŸ“¥ Data Ingestion â†’ `data_ingestion.py`
2. ğŸ§¼ Preprocessing â†’ `preprocessing.py`
3. ğŸ§  Feature Engineering â†’ `feature_engineering.py`
4. ğŸ”€ Train-Test Split â†’ `splitting.py`
5. ğŸ¤– Model Training (Multiple Models) â†’ `training.py`
6. âœ… Evaluation & Best Model Selection â†’ `evaluation.py`

---

## ğŸ” ZenML Pipelines

* Pipelines defined using `@step` and `@pipeline` decorators
* Integrated MLflow logging
* Best model is picked based on ROC-AUC

Run with:

```bash
zenml up
python run_pipeline.py
```

---

## âš™ï¸ Deployment (Streamlit + Docker + EC2)

* Github CI/CD pipeline deploys Streamlit app
* Docker container built with all artifacts
* Hosted on AWS EC2 with port exposed

**Inference:**

* User enters customer data on frontend
* Real-time prediction + confidence
* Saves inputs to DynamoDB

---

## âš ï¸ Drift Detection & Auto-Retraining (GitHub Actions)

* Uses reference data stored in `data/production/reference/`
* Every **30 days**:

  * GitHub Actions triggers `drift_detection.yml`
  * Runs Evidently to compare live vs reference data
  * If drift is detected: retrains the model using ZenML
  * Pushes new model artifacts to repo
  * Triggers build + deploy via CI/CD

---

## ğŸ” CI/CD â€“ GitHub Actions

* On every push to `main`:

  1. âœ… Run unit tests via `pytest`
  2. ğŸ³ Build Docker image
  3. ğŸš€ Push to DockerHub
  4. â˜ï¸ Deploy to EC2 server in cloud

---

## ğŸ§ª Testing

* Unit tests for every module
* Run with:

```bash
pytest tests/
```

---

## ğŸ’¾ Database Integration

* ğŸ›¢ï¸ DynamoDB used to store prediction logs
* Connected via `boto3`
* Used as data source for drift monitoring

---

## ğŸ“ˆ Metrics

Include ROC Curve, Confusion Matrix, Precision-Recall Curve, and F1/Accuracy from model evaluation.

![ROC Curve](artifacts/evaluation/roc_curve.png)
![Confusion Matrix](artifacts/evaluation/confusion_matrix.png)
![Precision Recall Curve](artifacts/evaluation/precision_recall_curve.png)

---

## ğŸš€ Tech Stack

| Area            | Tool                     |
| --------------- | ------------------------ |
| Orchestration   | ZenML                    |
| Model Tracking  | MLflow                   |
| Deployment      | Docker + Streamlit + EC2 |
| CI/CD           | GitHub Actions           |
| Drift Detection | Evidently                |
| Data Storage    | DynamoDB                 |
| Testing         | Pytest                   |

---

## ğŸ“– Project Setup & Installation Guide

Follow these steps to get this project running locally or in a cloud environment.

### âœˆï¸ Clone the Repository

```bash
git clone https://github.com/HimadeepRagiri/MLOps-Telecom-Churn-Detection.git
cd MLOps-Telecom-Churn-Detection
```

### ğŸ“ˆ Create & Activate a Virtual Environment

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### âœ¨ Install Dependencies

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

### ğŸ“„ Setup ZenML & Initialize Repository

```bash
zenml init
zenml integration install sklearn evidently mlflow -y
```

### âœï¸ Run the ZenML Pipeline Locally

```bash
python run_pipeline.py
```

This runs all the stages: ingestion, cleaning, feature engineering, training and evaluation.

### ğŸªœ Run Unit Tests

```bash
pytest tests/
```

### ğŸ™ï¸ Run Streamlit App (Optional, Local Testing)

```bash
streamlit run src/deployment.py
```

### ğŸš€ CI/CD and Docker Deployment

* GitHub Actions automatically runs on push to `main`:

  * Runs tests
  * Builds Docker image
  * Pushes image to Docker Hub
  * Deploys to AWS EC2 instance

---

### â° Drift Detection & Retraining

* Scheduled via GitHub Actions (cron job)
* Runs drift detection every 30 days using Evidently
* If drift detected:

  * Triggers full ZenML pipeline
  * Retrains model
  * Updates model artifacts
  * Pushes to `main` branch
  * GitHub Actions automatically deploys updated model

---

### ğŸŒ Optional Cloud Setup (Free Tier)

* **AWS EC2**: Host your Streamlit API on a t2.micro instance
* **Docker Hub**: Used to store and pull Docker images
* **GitHub Actions**: Automates deployment
* **DynamoDB**: Optional NoSQL database for storing prediction logs or new inference data

---

## ğŸš§ Future Enhancements

* Add support for GCP Cloud Run and Firestore
* Add Slack or email alert on drift
* Real-time feature store
* Experiment tracking UI (like MLflow UI hosted remotely)

---

## ğŸ“œ License

This project is licensed under the terms of the **MIT License**. See the [LICENSE](LICENSE) file for details.

---

## ğŸ™Œ Credits

* ZenML team
* Evidently AI
* Open-source community
* Dataset by IBM/Kaggle

---

## ğŸŒŸ Star the Repo

If you find this project useful, feel free to â­ï¸ it and share it with others!
